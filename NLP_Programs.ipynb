{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qWQ61ReBT4c",
        "outputId": "17244447-919e-46c6-eaf4-4ca26458d2cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **1. A python program to perform tokenization by word and sentence using nltk.**"
      ],
      "metadata": {
        "id": "einTvCAQ_X4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Program for sentence tokenization:**"
      ],
      "metadata": {
        "id": "LyUM4Nd7_qIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYnmiUZg_PpX",
        "outputId": "c80a96e0-227f-4a2c-ef2b-16d1b9b3f196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: NLTK is a leading platform for building Python programs to work with human language data.\n",
            "Sentence 2: It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the necessary tokenization models\n",
        "from nltk.tokenize import sent_tokenize\n",
        "def tokenize_sentences(text):\n",
        " sentences = sent_tokenize(text)\n",
        " return sentences\n",
        "# Example text\n",
        "text =\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\"\n",
        "# Tokenize sentences\n",
        "sentences = tokenize_sentences(text)\n",
        "# Print tokenized sentences\n",
        "for i, sentence in enumerate(sentences):\n",
        " print(f\"Sentence {i+1}: {sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Program for word Tokenization:**"
      ],
      "metadata": {
        "id": "aegKsT06_nfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Download the necessary tokenization models\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize_words(text):\n",
        " words = word_tokenize(text)\n",
        " return words\n",
        "# Example text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "# Tokenize words\n",
        "words = tokenize_words(text)\n",
        "# Print tokenized words\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DVxLNu__jwg",
        "outputId": "67254bfd-448c-45f9-e978-ae726af44790"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. A python program to eliminate stopwords using nltk**"
      ],
      "metadata": {
        "id": "sx5AQbmR_yhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download NLTK stopwords and tokenizer models\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def remove_stopwords(text):\n",
        " # Tokenize the text into words\n",
        " words = word_tokenize(text)\n",
        " # Get English stopwords\n",
        " english_stopwords = set(stopwords.words('english'))\n",
        " # Remove stopwords from the tokenized words\n",
        " filtered_words = [word for word in words if word.lower() not in english_stopwords]\n",
        " # Join the filtered words back into a single string\n",
        " filtered_text = ' '.join(filtered_words)\n",
        " return filtered_text\n",
        "# Example text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "# Remove stopwords\n",
        "filtered_text = remove_stopwords(text)\n",
        "# Print filtered text\n",
        "print(filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6cABrx3_0hq",
        "outputId": "fd2e0c32-db94-4e8d-acf2-a54d73cd8394"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK leading platform building Python programs work human language data .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. A python program to perform stemming using nltk.**"
      ],
      "metadata": {
        "id": "RzFLCCQp_28a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download NLTK tokenizer and stemmer models\n",
        "nltk.download('punkt')\n",
        "def stem_text(text):\n",
        " # Initialize the Porter Stemmer\n",
        " porter_stemmer = PorterStemmer()\n",
        " # Tokenize the text into words\n",
        " words = word_tokenize(text)\n",
        " # Apply stemming to each word\n",
        " stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        " # Join the stemmed words back into a single string\n",
        " stemmed_text = ' '.join(stemmed_words)\n",
        " return stemmed_text\n",
        "# Example text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "# Perform stemming\n",
        "stemmed_text = stem_text(text)\n",
        "# Print stemmed text\n",
        "print(stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3kCS8W9_7HV",
        "outputId": "0e1f84bd-7bb3-446f-b63c-b2a1bbd0f7a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk is a lead platform for build python program to work with human languag data .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. A python program to perform Parts of Speech tagging using nltk.**"
      ],
      "metadata": {
        "id": "ooIUESLpAE13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download NLTK tokenizer and POS tagging models\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Download the specific 'averaged_perceptron_tagger_eng' model\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "def pos_tagging(text):\n",
        " # Tokenize the text into words\n",
        " words = word_tokenize(text)\n",
        " # Perform POS tagging\n",
        " tagged_words = nltk.pos_tag(words)\n",
        " return tagged_words\n",
        "# Example text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "# Perform POS tagging\n",
        "tagged_text = pos_tagging(text)\n",
        "# Print POS tagged text\n",
        "print(tagged_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF8gL0-NANxF",
        "outputId": "e626bdc1-60b3-4c03-e21d-6f3092788dc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. A python program to perform lemmatization using nltk.**"
      ],
      "metadata": {
        "id": "1vpc9owmARTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "  return lemmatized_text\n",
        "\n",
        "text = \"The cats are chasing mice and playing in the garden\"\n",
        "lemmatized_text = lemmatize_text(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VecwqnwlAV1r",
        "outputId": "506ec464-9915-421f-e566-a44920463594"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The cats are chasing mice and playing in the garden\n",
            "Lemmatized Text: The cat are chasing mouse and playing in the garden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. A python program for chunking using nltk.**"
      ],
      "metadata": {
        "id": "1s45-56_Ab73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, RegexpParser\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def chunk_sentence(sentence):\n",
        "  words = word_tokenize(sentence)\n",
        "  tagged_words = pos_tag(words)\n",
        "\n",
        "  # Define grammar for chunking\n",
        "  grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}\n",
        "  # Chunk sequences of DT, JJ, NN\n",
        "  PP: {<IN><NP>}\n",
        "  # Chunk prepositions followed by NP\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
        "  # Chunk verbs and their arguments\n",
        "  CLAUSE: {<NP><VP>}\n",
        "  # Chunk NP, VP pairs\n",
        "  \"\"\"\n",
        "  parser = RegexpParser(grammar)\n",
        "  chunked_sentence = parser.parse(tagged_words)\n",
        "\n",
        "  return chunked_sentence\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "chunked_sentence = chunk_sentence(sentence)\n",
        "print(chunked_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gVGslq-AYxF",
        "outputId": "4f69f1aa-6b2c-423b-cfa7-8e866b839b5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (CLAUSE\n",
            "    (NP The/DT quick/JJ brown/NN fox/NN)\n",
            "    (VP jumps/VBZ (PP over/IN (NP the/DT lazy/JJ dog/NN)))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. A python program to perform Named Entity Recognition using nltk.**"
      ],
      "metadata": {
        "id": "8tc92iBjAjZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "def ner(text):\n",
        "  words = word_tokenize(text)\n",
        "  tagged_words = pos_tag(words)\n",
        "  named_entities = ne_chunk(tagged_words)\n",
        "  return named_entities\n",
        "\n",
        "text = \"Apple is a company based in California, United States. Steve Jobs was one of its founders.\"\n",
        "named_entities = ner(text)\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hmCp5HeApW0",
        "outputId": "d7b693cb-01e3-4304-f342-4450a77c02de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  company/NN\n",
            "  based/VBN\n",
            "  in/IN\n",
            "  (GPE California/NNP)\n",
            "  ,/,\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.\n",
            "  (PERSON Steve/NNP Jobs/NNP)\n",
            "  was/VBD\n",
            "  one/CD\n",
            "  of/IN\n",
            "  its/PRP$\n",
            "  founders/NNS\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. A python program to find Term Frequency and Inverse Document Frequency (TF-IDF).**"
      ],
      "metadata": {
        "id": "PuRR7PksAzNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "# Tokenize and preprocess the documents\n",
        "def preprocess_text(doc):\n",
        "    # Tokenize the document into words\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Convert words to lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    preprocessed_doc = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_doc\n",
        "\n",
        "# Preprocess all documents\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "# Compute TF-IDF scores using scikit-learn\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "# Print TF-IDF matrix\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEO9R0ivA14-",
        "outputId": "4e59e7d0-8e28-4cfd-e48c-99420d41e217"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.62922751 0.77722116 0.         0.         0.        ]\n",
            " [0.78722298 0.         0.         0.61666846 0.        ]\n",
            " [0.         0.         0.70710678 0.         0.70710678]\n",
            " [0.62922751 0.77722116 0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. A python program for CYK parsing (Cocke-Younger-Kasami Parsing) or Chart Parsing.**"
      ],
      "metadata": {
        "id": "tcFSK_wvA5Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> V NP\n",
        "V -> 'describe' | 'present'\n",
        "NP -> PRP N\n",
        "PRP -> 'your'\n",
        "N -> 'work'\n",
        "\"\"\")\n",
        "parser = nltk.ChartParser(grammar)\n",
        "sent = 'describe your work'.split()\n",
        "print (list(parser.parse(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPKjdG1oA4zA",
        "outputId": "064765e7-7c1e-4c13-8e43-d0c15e9ed878"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tree('S', [Tree('V', ['describe']), Tree('NP', [Tree('PRP', ['your']), Tree('N', ['work'])])])]\n"
          ]
        }
      ]
    }
  ]
}